{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3q5xEYi_BzJ"
      },
      "outputs": [],
      "source": [
        "# Course link\n",
        "# notebook\n",
        "#  https://github.com/codebasics/llm-fine-tuning-crash-course/blob/main/2_unsloth_finetuning.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q unsloth\n",
        "# !pip install -q --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ],
      "metadata": {
        "id": "BvjyxxqeANN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 20148\n",
        "dtype=None\n",
        "load_in_4bit=True\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdHn-vEGANLB",
        "outputId": "a0425277-8954-49e1-9a37-23e73f33472d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-3B-Instruct\",  # \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMO9F4xaANIv",
        "outputId": "18273b9b-9ce0-4624-afcf-9f02ff173302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.12.9: Fast Llama patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dtype: which specifies the model weigths and computations.\n",
        "\n",
        "None- automatically slects the appropriate data type based on the hardware\n",
        "\n",
        "\n",
        "*torch.float16-- uses 16bit floating point precision, reducing memory usage and potentially increasing speed on compatible GPUs*\n",
        "\n",
        "\n",
        "**torch.bfloat16 --- similar to float16 but with a weider dynamic range, beneficial for certain hardware like NVIDIA A100 GPUs**\n",
        "\n",
        "**load_in_4bit - determines whether to load the model using 4bit quantization . ideal for sceanrios where memory efficiency is crucial, such as deploying models on edge devices or during experimentation**\n"
      ],
      "metadata": {
        "id": "UHXOp57rEqOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now we be using get_peft_model from unsloth FastLangugaeModel class to attach\n",
        "#  adapters(peft layers) on top of the models in order to perform QLoRA\n"
      ],
      "metadata": {
        "id": "RcPJftvrANG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16, # we can choose any number like 8,16,32,64,128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16, # higher alpha value assigns more weight to the LoRA activations\n",
        "    lora_dropout= 0,\n",
        "    bias='none',\n",
        "    use_gradient_checkpointing='unsloth', # true or unsloth for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njeYT37uANET",
        "outputId": "1c8fdc46-eaaa-4f52-b446-1c7fcf12dfb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.12.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   **r**: the rank of low rank metrics in LoRA; higher alues can capture more information but increase memory usage\n",
        "2.   **target_modules**: list of model components where LoRA adapters are inserted for fientuning\n",
        "3. **lora_alpha**; scaling factor factor for the lora updates, controls the impact of the adapters on the models outputs\n",
        "4. **lora_dropout**; dropout rate applies to lora layers during training to prevent overfitting\n",
        "5. **use_gradient_checkpointing**; enables gradient checkpointing to reduce the memory usage during training, unsloth use unsloths optimized version\n",
        "6. **use_rslora**; boolean indicating whether to use rank stablized LoRA (rsLoRA) for potentiallly more stable training\n",
        "\n"
      ],
      "metadata": {
        "id": "-JUD4WlFIRvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset= load_dataset(\"ServiceNow-AI/R1-Distill-SFT\",'v0',split='train')"
      ],
      "metadata": {
        "id": "LQa2xyrAANBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WlWzxKDAM_T",
        "outputId": "7b803401-2ac9-42f0-e3f0-ed08bfd48a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'reannotated_assistant_content', 'problem', 'source', 'solution', 'verified', 'quality_metrics'],\n",
            "    num_rows: 171647\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['reannotated_assistant_content'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "sUXL0SmJAM83",
        "outputId": "7a1dcf33-3b08-407f-b253-976ed416e036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<think>\\nFirst, I need to determine the total number of children on the playground by adding the number of boys and girls.\\n\\nThere are 27 boys and 35 girls.\\n\\nAdding these together: 27 boys + 35 girls = 62 children.\\n\\nTherefore, the total number of children on the playground is 62.\\n</think>\\n\\nTo find the total number of children on the playground, we simply add the number of boys and girls together.\\n\\n\\\\[\\n\\\\text{Total children} = \\\\text{Number of boys} + \\\\text{Number of girls}\\n\\\\]\\n\\nPlugging in the given values:\\n\\n\\\\[\\n\\\\text{Total children} = 27 \\\\text{ boys} + 35 \\\\text{ girls} = 62 \\\\text{ children}\\n\\\\]\\n\\n**Final Answer:**\\n\\n\\\\[\\n\\\\boxed{62}\\n\\\\]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['reannotated_assistant_content'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNspELGIAM6i",
        "outputId": "75aed768-4eca-460e-9eb5-0c12e6f24cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "First, I need to determine the total number of children on the playground by adding the number of boys and girls.\n",
            "\n",
            "There are 27 boys and 35 girls.\n",
            "\n",
            "Adding these together: 27 boys + 35 girls = 62 children.\n",
            "\n",
            "Therefore, the total number of children on the playground is 62.\n",
            "</think>\n",
            "\n",
            "To find the total number of children on the playground, we simply add the number of boys and girls together.\n",
            "\n",
            "\\[\n",
            "\\text{Total children} = \\text{Number of boys} + \\text{Number of girls}\n",
            "\\]\n",
            "\n",
            "Plugging in the given values:\n",
            "\n",
            "\\[\n",
            "\\text{Total children} = 27 \\text{ boys} + 35 \\text{ girls} = 62 \\text{ children}\n",
            "\\]\n",
            "\n",
            "**Final Answer:**\n",
            "\n",
            "\\[\n",
            "\\boxed{62}\n",
            "\\]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.column_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auyN2R6_AM4I",
        "outputId": "d259f235-159e-4259-d05d-99a8587abd56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id',\n",
              " 'reannotated_assistant_content',\n",
              " 'problem',\n",
              " 'source',\n",
              " 'solution',\n",
              " 'verified',\n",
              " 'quality_metrics']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col in dataset.column_names:\n",
        "  if col!='id':\n",
        "    print('---'*10,'\\n', col,'\\n','---'*10)\n",
        "    print(dataset[col][20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxD27ZMfAM1d",
        "outputId": "43c5ad2b-a186-4654-96fc-0a4c66924f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------ \n",
            " reannotated_assistant_content \n",
            " ------------------------------\n",
            "<think>\n",
            "First, calculate the number of correct answers Liza got by finding 90% of 60.\n",
            "\n",
            "Next, determine Rose's correct answers by adding 2 to Liza's correct answers.\n",
            "\n",
            "Finally, subtract the number of correct answers Rose got from the total number of items to find the number of incorrect answers.\n",
            "</think>\n",
            "\n",
            "**Solution:**\n",
            "\n",
            "1. **Calculate Liza's Correct Answers:**\n",
            "\n",
            "   Liza got 90% of the 60 items correct.\n",
            "   \n",
            "   \\[\n",
            "   \\text{Correct Answers (Liza)} = 0.90 \\times 60 = 54\n",
            "   \\]\n",
            "\n",
            "2. **Determine Rose's Correct Answers:**\n",
            "\n",
            "   Rose got 2 more correct answers than Liza.\n",
            "   \n",
            "   \\[\n",
            "   \\text{Correct Answers (Rose)} = 54 + 2 = 56\n",
            "   \\]\n",
            "\n",
            "3. **Find Rose's Incorrect Answers:**\n",
            "\n",
            "   The total number of items is 60. To find the number of incorrect answers Rose had:\n",
            "   \n",
            "   \\[\n",
            "   \\text{Incorrect Answers (Rose)} = 60 - 56 = \\boxed{4}\n",
            "   \\]\n",
            "\n",
            "**Answer:** Rose had \\(\\boxed{4}\\) incorrect answers.\n",
            "------------------------------ \n",
            " problem \n",
            " ------------------------------\n",
            "In a 60-item exam, Liza got 90% of the items correctly. Her best friend, Rose, got 2 more correct answers than her. How many incorrect answers did Rose have?\n",
            "------------------------------ \n",
            " source \n",
            " ------------------------------\n",
            "orca_math\n",
            "------------------------------ \n",
            " solution \n",
            " ------------------------------\n",
            "If Liza got 90% of the items correctly in a 60-item exam, she got:\n",
            "\n",
            "90% of 60 = 0.90 * 60 = 54 correct answers.\n",
            "\n",
            "Rose got 2 more correct answers than Liza, so Rose got:\n",
            "\n",
            "54 + 2 = 56 correct answers.\n",
            "\n",
            "The total number of items in the exam is 60, so the number of incorrect answers Rose had is:\n",
            "\n",
            "60 - 56 = $\\boxed{4}$  incorrect answers.\n",
            "------------------------------ \n",
            " verified \n",
            " ------------------------------\n",
            "None\n",
            "------------------------------ \n",
            " quality_metrics \n",
            " ------------------------------\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# r1_prompt=\"\"\" You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
        "# <problem>\n",
        "# {}\n",
        "# </problem>\n",
        "\n",
        "# {}\n",
        "# {}\n",
        "# \"\"\"\n",
        "\n",
        "# EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# def formatting_prompts_func(examples):\n",
        "#   problems = examples['problem']\n",
        "#   thoughts = examples[\"reannotated_assistant_content\"]\n",
        "#   solutions = examples[\"solution\"]\n",
        "#   texts = []\n",
        "#   for problem, thought, solution in zip(problems, thoughts, solutions):\n",
        "#     text = r1_prompt.format(problem, thought, solution)+EOS_TOKEN\n",
        "#     texts.append(text)\n",
        "\n",
        "#   return {'text':texts}\n",
        "# datasets = dataset.map(formatting_prompts_func, batched=True)\n"
      ],
      "metadata": {
        "id": "Q-5YsMjcL2bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r1_prompt = \"\"\"You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
        "<problem>\n",
        "{}\n",
        "</problem>\n",
        "\n",
        "{}\n",
        "{}\n",
        "\"\"\"\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "  problems = examples[\"problem\"]\n",
        "  thoughts = examples[\"reannotated_assistant_content\"]\n",
        "  solutions = examples[\"solution\"]\n",
        "  texts = []\n",
        "\n",
        "  for problem, thought, solution in zip(problems, thoughts, solutions):\n",
        "    text = r1_prompt.format(problem, thought, solution)+EOS_TOKEN\n",
        "    texts.append(text)\n",
        "\n",
        "  return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "-qsbz6vzW1q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.column_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B05oxeMXPRlD",
        "outputId": "d603f90d-918c-44a1-9ff8-7336f2f4846f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id',\n",
              " 'reannotated_assistant_content',\n",
              " 'problem',\n",
              " 'source',\n",
              " 'solution',\n",
              " 'verified',\n",
              " 'quality_metrics',\n",
              " 'text']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col in dataset.column_names:\n",
        "  if col!='id':\n",
        "    print('---'*10,'\\n', col,'\\n','---'*10)\n",
        "    print(dataset[col][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRAMQ_mDO6Wt",
        "outputId": "54007e87-ac00-443b-c08a-4a5ffd8e5292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------ \n",
            " reannotated_assistant_content \n",
            " ------------------------------\n",
            "<think>\n",
            "First, determine how many bags Bianca actually recycled by subtracting the number of bags she did not recycle from the total number of bags she had.\n",
            "\n",
            "Then, calculate the total points she earned by multiplying the number of recycled bags by the points earned per bag.\n",
            "</think>\n",
            "\n",
            "**Solution:**\n",
            "\n",
            "1. **Determine the number of bags Bianca recycled:**\n",
            "\n",
            "   Bianca had a total of 17 bags but did not recycle 8 of them. Therefore, the number of bags she actually recycled is:\n",
            "   \n",
            "   \\[\n",
            "   17 \\text{ bags} - 8 \\text{ bags} = 9 \\text{ bags}\n",
            "   \\]\n",
            "\n",
            "2. **Calculate the total points earned:**\n",
            "\n",
            "   Bianca earns 5 points for each bag she recycles. For 9 bags, the total points earned are:\n",
            "   \n",
            "   \\[\n",
            "   9 \\text{ bags} \\times 5 \\text{ points/bag} = 45 \\text{ points}\n",
            "   \\]\n",
            "\n",
            "**Final Answer:**\n",
            "\n",
            "\\[\n",
            "\\boxed{45}\n",
            "\\]\n",
            "------------------------------ \n",
            " problem \n",
            " ------------------------------\n",
            "Bianca earned 5 points for each bag of cans she recycled. If she had 17 bags, but didn't recycle 8 of them, how many points would she have earned?\n",
            "------------------------------ \n",
            " source \n",
            " ------------------------------\n",
            "orca_math\n",
            "------------------------------ \n",
            " solution \n",
            " ------------------------------\n",
            "Bianca recycled 17 - 8 = 9 bags of cans.\n",
            "\n",
            "For each bag of cans, she earned 5 points, so for 9 bags, she would have earned 9 * 5 = $\\boxed{45}$  points.\n",
            "------------------------------ \n",
            " verified \n",
            " ------------------------------\n",
            "None\n",
            "------------------------------ \n",
            " quality_metrics \n",
            " ------------------------------\n",
            "None\n",
            "------------------------------ \n",
            " text \n",
            " ------------------------------\n",
            "You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
            "<problem>\n",
            "Bianca earned 5 points for each bag of cans she recycled. If she had 17 bags, but didn't recycle 8 of them, how many points would she have earned?\n",
            "</problem>\n",
            "\n",
            "<think>\n",
            "First, determine how many bags Bianca actually recycled by subtracting the number of bags she did not recycle from the total number of bags she had.\n",
            "\n",
            "Then, calculate the total points she earned by multiplying the number of recycled bags by the points earned per bag.\n",
            "</think>\n",
            "\n",
            "**Solution:**\n",
            "\n",
            "1. **Determine the number of bags Bianca recycled:**\n",
            "\n",
            "   Bianca had a total of 17 bags but did not recycle 8 of them. Therefore, the number of bags she actually recycled is:\n",
            "   \n",
            "   \\[\n",
            "   17 \\text{ bags} - 8 \\text{ bags} = 9 \\text{ bags}\n",
            "   \\]\n",
            "\n",
            "2. **Calculate the total points earned:**\n",
            "\n",
            "   Bianca earns 5 points for each bag she recycles. For 9 bags, the total points earned are:\n",
            "   \n",
            "   \\[\n",
            "   9 \\text{ bags} \\times 5 \\text{ points/bag} = 45 \\text{ points}\n",
            "   \\]\n",
            "\n",
            "**Final Answer:**\n",
            "\n",
            "\\[\n",
            "\\boxed{45}\n",
            "\\]\n",
            "Bianca recycled 17 - 8 = 9 bags of cans.\n",
            "\n",
            "For each bag of cans, she earned 5 points, so for 9 bags, she would have earned 9 * 5 = $\\boxed{45}$  points.\n",
            "<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Trainer SETUP**"
      ],
      "metadata": {
        "id": "f1JoNZDkPqJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset_text_field ; specifies the field in\n",
        "the dataset that contains the text data\n",
        "\n",
        "max_seq_length ; maximum length for the inout data\n",
        "\n",
        "\n",
        "dataset_num_proc ; number of processes to use for data loading\n",
        "\n",
        "packing; if True, enables sequence packing (concatenates multiple examples into a single sequence to better utlize tokens)"
      ],
      "metadata": {
        "id": "PtxqOpXSPyPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Arguments; **\n",
        "\n",
        "- per_device-train-batchsize ; number of samples\n",
        "per batch for each device\n",
        "\n",
        "- gradient accumlation steps; number of steps to acculate gradients before updating model weigths\n",
        "\n",
        "- warmup steps - number of steps for training steps\n",
        "\n",
        "- learning rate - learning rate for optimizer\n",
        "\n",
        "- logging steps - frequenct of logging training progresss\n",
        "\n",
        "- optim  optimizer type here using an 8 bit version of adamW\n",
        "\n",
        "- weight decay - ragularization parameter for weight decay\n",
        "\n",
        "- lr scheduler type - type of learning rate scheduler\n",
        "\n",
        "- seed - random seed for reproducibility\n",
        "\n",
        "- report to - intergation for observability tools like wanbd, tensorboard etc"
      ],
      "metadata": {
        "id": "lSZHBMW0Qdjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U trl\n"
      ],
      "metadata": {
        "id": "oRcwgFu4YuSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install -U psutil\n"
      ],
      "metadata": {
        "id": "97Mah7iQbkr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import AutoTokenizer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "import psutil\n"
      ],
      "metadata": {
        "id": "RJgFG9kjPw9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth_compiled_cache.UnslothSFTTrainer as ust\n",
        "\n",
        "ust.psutil = psutil"
      ],
      "metadata": {
        "id": "o2TCVqspcX_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2, # Number of processors to use for processing the dataset\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2, # The batch size per GPU/TPU core\n",
        "        gradient_accumulation_steps = 4, # Number of steps to perform befor each gradient accumulation\n",
        "        warmup_steps = 5, # Few updates with low learning rate before actual training\n",
        "        max_steps = 60, # Specifies the total number of training steps (batches) to run.\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\", # Optimizer\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        dataloader_num_workers=2,\n",
        "        report_to = \"none\", # Use this for WandB etc for observability\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "BwarGIxXWWzG",
        "outputId": "ea47abe1-d2c5-4f51-a9c1-2213ef6ac721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'psutil' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-355777694.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer = SFTTrainer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdataset_text_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/trainer.py\u001b[0m in \u001b[0;36mnew_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0moriginal_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpacking_active\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_should_skip_auto_packing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/trainer.py\u001b[0m in \u001b[0;36mnew_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0moriginal_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func, **kwargs)\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_gradient_checkpointing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gradient_checkpointing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m   1419\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    883\u001b[0m                     \u001b[0;34m\"dataset, or disable `completion_only_loss` in `SFTConfig`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                 )\n\u001b[0;32m--> 885\u001b[0;31m             train_dataset = self._prepare_dataset(\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessing_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpacking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatting_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             )\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothSFTTrainer.py\u001b[0m in \u001b[0;36m_prepare_dataset\u001b[0;34m(self, dataset, processing_class, args, packing, formatting_func, dataset_name)\u001b[0m\n\u001b[1;32m   1056\u001b[0m                 \u001b[0mdataset_num_proc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dataset_num_proc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdataset_num_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m                     \u001b[0mdataset_num_proc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m                     \u001b[0;31m# Check memory left so we can reduce multiprocessing to converse memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m                     \u001b[0mmemory_gb_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'psutil' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "xxJFgv5aeUuM",
        "outputId": "1650cc48-a4d5-491d-f042-dee31351713b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-773422404.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "sys_prompt = \"\"\"\n",
        "You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
        "<problem>\n",
        "{}\n",
        "</problem>\n",
        "\"\"\"\n",
        "message= sys_prompt.format(\"How many r's are present in strawberry\")\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,chat_template =\"llama-3.1\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # enable native 2X faster inference\n",
        "\n",
        "messages= [\n",
        "    {'role':'user',\n",
        "     'content':message},\n",
        "]\n",
        "inputs= tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors='pt',\n",
        ").to('cuda')\n",
        "\n",
        "outputs=model.generate(input_id= inputs,\n",
        "                       max_new_tokens= 1024,\n",
        "                       use_cache= True,\n",
        "                       temperature = 1.5, min_p = 0.1)\n",
        "response= tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "mHuj7POpiPSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[0])"
      ],
      "metadata": {
        "id": "VEj96prDmeRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('model_1_mk')\n",
        "tokenizer.save_pretrained('model_2_m')"
      ],
      "metadata": {
        "id": "J4wgJrpymeOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_gguf('chatGGUF',tokenizer)"
      ],
      "metadata": {
        "id": "76vj5pTlmeL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KDyLPMH_nTxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the model via Ollama (OPTIONAL)"
      ],
      "metadata": {
        "id": "A5M80z_qnTqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "QVBLCsxQmeJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "zctEhol-meG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer._ollama_modelfile)"
      ],
      "metadata": {
        "id": "Tna0KHQhnQ56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!ollama create unsloth_model -f ./chatGGUF/Modelfile"
      ],
      "metadata": {
        "id": "8V_tw24TmeD_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}